{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Experimentation V2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Paper Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the paperscraper module\n",
    "from paperscraper import paperscraper\n",
    "\n",
    "# Create an instance of the class\n",
    "scraper = paperscraper('2024-10-04')\n",
    "\n",
    "# Get sections of AIAYN paper using html_subdivide function\n",
    "AIAYN_sections = scraper.html_subdivide('https://arxiv.org/html/1706.03762v7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summarisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Import os\n",
    "import os\n",
    "\n",
    "# Import GPT model and tokenizer\n",
    "os.environ['HF_HOME'] = r'C:\\Users\\josha\\AppData\\Local\\Temp'        # Set cache directory for HuggingFace\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration    # Using Google FLAN-T5\n",
    "\n",
    "# Create an instance of the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Create an instance of the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summarisation function\n",
    "def section_summarizer(text):\n",
    "    # Define prompt\n",
    "    prompt = f'''You are an AI assistant capable of summarizing academic content on AI and Machine Learning for non-academic readers. Your summaries are given in the third-person, and are about 2-3 sentences in length.\n",
    "    Summarize the following abstract:\n",
    "    {text}'''\n",
    "\n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        input_ids,                  # Input ids\n",
    "        max_length = 500,           # Maximum number of tokens to generate\n",
    "        num_beams = 5,              # Needed as next token is found using beam search\n",
    "        no_repeat_ngram_size = 2,   # Stops model from repeating word sequences repeatedly\n",
    "        early_stopping = False,      # If output becomes not very good, stop generating\n",
    "    )\n",
    "\n",
    "    # Return the decoded the output\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6070 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "summarised_sections = {}\n",
    "\n",
    "for section, content in AIAYN_sections.items():\n",
    "    summarised_sections[section] = section_summarizer(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': 'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2.',\n",
       " 'Introduction': 'We propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.',\n",
       " 'Background': 'The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.',\n",
       " 'Model Architecture': 'We propose an encoder-decoder architecture based on a multi-head self-attention and point-wise fully connected feed-forward networks.',\n",
       " 'Encoder and Decoder Stacks': 'We employ a residual connection around each of the two sub-layers, followed by layer normalization. We also modify the self-attention sublayer in the decoder stack to prevent positions from attending to subsequent positions.',\n",
       " 'Attention': 'We propose a multi-head attention function for encoding and decoders.',\n",
       " 'Position-wise Feed-Forward Networks': 'Our encoder and decoder contain a fully connected feed-forward network, which is applied to each position separately and identically. The linear transformations are the same across different positions, and they use different parameters from layer to layer.',\n",
       " 'Embeddings and Softmax': 'We use learned embeddings to convert decoder output to predicted next-token probabilities.',\n",
       " 'Positional Encoding': 'We add positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks.',\n",
       " 'Why Self-Attention': 'We compare self-attention layers to the recurrent and convolutional layers for learning a variable-length sequence of symbol representations.',\n",
       " 'Training': 'We train our models on the standard WMT 2014 English-German dataset using byte-pair encoding[3].',\n",
       " 'Training Data and Batching': 'We trained on the standard WMT 2014 English-German dataset using byte-pair encoding[3].',\n",
       " 'Hardware and Schedule': 'We trained our models on one machine with 8 NVIDIA P100 GPUs. Each training step took about 0.4 seconds.',\n",
       " 'Optimizer': 'We used the Adam optimizer[20] with 1=0.9 subscript10.9beta_ 1=0.7italic start_POSTSUBSCRIPT 1 end_PSTSBRACPIT = 0.9,2 =0.98bd model20.98bytesate=dmodel0.5min(step_num 0.5,sstep\\xadnum1.5)lratesuperscriptsubscriptdModel0.5step_nuSuperscriptm0.5>step numwarmup_step superscripts1.5l rate=textmodel[d]x1.0cdot-min(ascii_s[0][1]for x,y,z,p,t,e,i,j,k,l,r,m,n,u,italic_p[2] for y and z) for the first warmup steps of training.',\n",
       " 'Regularization': 'We apply dropout[33]to the output of each sub-layer, before it is added to the sublayer input and normalized. For the base model, we use a rate ofPdrop=0.1subscriptPdrop0.1P_drop=0.2.',\n",
       " 'Results': 'The Transformer outperforms RNN sequence-to-sequence models in English constituency parsing.',\n",
       " 'Machine Translation': 'The Transformer achieves better BLEU scores than previous state-of-the-art models, at a fraction of the training cost. On the WMT 2014 English-to-German translation task, our big transformer model outperforms all previously published models.',\n",
       " 'Model Variations': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. We present our results in Table3. In Table3rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
       " 'English Constituency Parsing': 'The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)ParserTraining.',\n",
       " 'Conclusion': 'We present the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks.',\n",
       " 'References': '[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Neural machine translation by jointly learning to align and translate.CoRR, abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong,',\n",
       " 'Attention Visualizations': 'We present two examples of the attention mechanism following long-distance dependencies in the encoder self-attention at layer 5 of 6.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarised_sections"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
