{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Install & Load Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.45.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.25.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\josha\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2023.7.22)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install transformers\n",
    "%pip install transformers\n",
    "\n",
    "# Import os\n",
    "import os\n",
    "\n",
    "# Import GPT model and tokenizer\n",
    "os.environ['HF_HOME'] = r'C:\\Users\\josha\\AppData\\Local\\Temp'        # Set cache directory for HuggingFace\n",
    "#from transformers import GPT2LMHeadModel, GPT2Tokenizer            # Using GPT2\n",
    "#from transformers import AutoModelForCausalLM, AutoTokenizer       # Using microsoft phi\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration    # Using Google FLAN-T5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the tokenizer\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2-large')\n",
    "#tokenizer = AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-128k-instruct')\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "\n",
    "# Create an instance of the model\n",
    "#model = GPT2LMHeadModel.from_pretrained('gpt2-large', pad_token_id=tokenizer.eos_token_id)\n",
    "#model = AutoModelForCausalLM.from_pretrained('microsoft/Phi-3-mini-128k-instruct', pad_token_id=tokenizer.eos_token_id)\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", pad_token_id=tokenizer.eos_token_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Define Summarizer Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def section_summarizer(text):\n",
    "    # Define prompt\n",
    "    prompt = f'''You are an AI assistant capable of summarizing academic content on AI and Machine Learning for non-academic readers. Your summaries are given in the third-person, and are about 2-3 sentences in length.\n",
    "    Summarize the following abstract:\n",
    "    {text}'''\n",
    "\n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        input_ids,                  # Input ids\n",
    "        max_length = 500,           # Maximum number of tokens to generate\n",
    "        num_beams = 5,              # Needed as next token is found using beam search\n",
    "        no_repeat_ngram_size = 2,   # Stops model from repeating word sequences repeatedly\n",
    "        early_stopping = False,      # If output becomes not very good, stop generating\n",
    "    )\n",
    "\n",
    "    # Return the decoded the output\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Test Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "AIAYN_abstract = '''The dominant sequence transduction models are based on complex recurrent or\n",
    " convolutional neural networks that include an encoder and a decoder. The best\n",
    " performing models also connect the encoder and decoder through an attention\n",
    " mechanism. We propose a new simple network architecture, the Transformer,\n",
    " based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
    " entirely. Experiments on two machine translation tasks show these models to\n",
    " be superior in quality while being more parallelizable and requiring significantly\n",
    " less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English\n",
    "to-German translation task, improving over the existing best results, including\n",
    " ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
    " our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
    " training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
    " best models from the literature. We show that the Transformer generalizes well to\n",
    " other tasks by applying it successfully to English constituency parsing both with\n",
    " large and limited training data.'''\n",
    "\n",
    "summary = section_summarizer(AIAYN_abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Our model achieves 28.4 BLEU on the WMT 2014 English to-German translation task, improving over the existing best results, including ensembles, by over 2.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
