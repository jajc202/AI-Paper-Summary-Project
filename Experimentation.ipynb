{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Experimentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Relevant Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Webpage for a date's (say 4th October 2024) daily papers has the format: https://huggingface.co/papers?date=2024-10-04\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/papers/2410.02740\n",
      "PDF link not found for: Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models\n",
      "/papers/2410.02713\n",
      "PDF link not found for: Video Instruction Tuning With Synthetic Data\n",
      "/papers/2410.02757\n",
      "PDF link not found for: Loong: Generating Minute-level Long Videos with Autoregressive Language Models\n",
      "/papers/2410.02712\n",
      "PDF link not found for: LLaVA-Critic: Learning to Evaluate Multimodal Models\n",
      "/papers/2410.02746\n",
      "PDF link not found for: Contrastive Localized Language-Image Pre-Training\n",
      "/papers/2410.02073\n",
      "PDF link not found for: Depth Pro: Sharp Monocular Metric Depth in Less Than a Second\n",
      "/papers/2410.01679\n",
      "PDF link not found for: VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment\n",
      "/papers/2410.02724\n",
      "PDF link not found for: Large Language Models as Markov Chains\n",
      "/papers/2410.02678\n",
      "PDF link not found for: Distilling an End-to-End Voice Assistant Without Instruction Training Data\n",
      "/papers/2410.02416\n",
      "PDF link not found for: Eliminating Oversaturation and Artifacts of High Guidance Scales in Diffusion Models\n",
      "/papers/2409.19291\n",
      "PDF link not found for: CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling\n",
      "/papers/2410.02749\n",
      "PDF link not found for: Training Language Models on Synthetic Edit Sequences Improves Code Synthesis\n",
      "/papers/2410.02367\n",
      "PDF link not found for: SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration\n",
      "/papers/2410.02103\n",
      "PDF link not found for: MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis\n",
      "/papers/2410.02115\n",
      "PDF link not found for: L-CiteEval: Do Long-Context Models Truly Leverage Context for Responding?\n",
      "/papers/2410.02763\n",
      "PDF link not found for: Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos\n",
      "/papers/2410.02458\n",
      "PDF link not found for: MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation\n",
      "/papers/2410.02762\n",
      "PDF link not found for: Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations\n",
      "/papers/2410.02052\n",
      "PDF link not found for: Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning\n",
      "/papers/2410.02536\n",
      "PDF link not found for: Intelligence at the Edge of Chaos\n",
      "/papers/2410.02426\n",
      "PDF link not found for: Learning the Latent Rules of a Game from Data: A Chess Story\n",
      "/papers/2410.02056\n",
      "PDF link not found for: Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data\n",
      "/papers/2410.02525\n",
      "PDF link not found for: Contextual Document Embeddings\n",
      "/papers/2410.01335\n",
      "PDF link not found for: Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models\n",
      "/papers/2410.01946\n",
      "PDF link not found for: SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics\n",
      "/papers/2410.00255\n",
      "PDF link not found for: Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning\n",
      "/papers/2410.01782\n",
      "PDF link not found for: Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "# Base URL for Hugging Face\n",
    "BASE_URL = \"https://huggingface.co\"\n",
    "\n",
    "# URL for the Hugging Face daily papers page (update this to the correct page)\n",
    "URL = f\"{BASE_URL}/papers?date=2024-10-04\"\n",
    "\n",
    "# Function to extract PDF links from individual paper page\n",
    "def get_pdf_link(paper_page_url):\n",
    "    # Fetch the paper's page content\n",
    "    paper_response = requests.get(paper_page_url)\n",
    "    \n",
    "    # Parse the paper's page content\n",
    "    if paper_response.status_code == 200:\n",
    "        paper_soup = BeautifulSoup(paper_response.content, 'html.parser')\n",
    "        \n",
    "        # Find the PDF link on the paper's page\n",
    "        # Assuming the PDF link is in an <a> tag with 'href' that contains '.pdf'\n",
    "        pdf_link = paper_soup.find('a', href=lambda href: href and \".pdf\" in href)\n",
    "        \n",
    "        if pdf_link:\n",
    "            return pdf_link['href']  # Return the PDF link\n",
    "    return None\n",
    "\n",
    "# Function to download the PDF file\n",
    "def download_pdf(pdf_url, paper_title):\n",
    "    # Get the PDF content\n",
    "    pdf_response = requests.get(pdf_url)\n",
    "    \n",
    "    if pdf_response.status_code == 200:\n",
    "        # Define the PDF file name, making it a valid file name\n",
    "        paper_title = paper_title.replace(\"/\", \"-\").replace(\"\\\\\", \"-\")  # Clean file name\n",
    "        pdf_file_name = f\"{paper_title}.pdf\"\n",
    "        \n",
    "        # Write the PDF content to a file\n",
    "        with open(pdf_file_name, 'wb') as f:\n",
    "            f.write(pdf_response.content)\n",
    "        print(f\"Downloaded: {pdf_file_name}\")\n",
    "    else:\n",
    "        print(f\"Failed to download PDF: {pdf_url}\")\n",
    "\n",
    "# Step 1: Scrape the daily papers page for paper links\n",
    "response = requests.get(URL)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Parse the main daily papers page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find all the paper links\n",
    "    # We are looking for <a> tags with the given class attributes in the example you shared\n",
    "    paper_links = soup.find_all('a', class_='line-clamp-3 cursor-pointer text-balance')\n",
    "    \n",
    "    for paper in paper_links:\n",
    "        paper_title = paper.text.strip()  # Get the title of the paper\n",
    "        paper_page_url = paper['href']   # Get the relative link to the paper's Hugging Face page\n",
    "\n",
    "        print(paper_page_url)\n",
    "        \n",
    "        # Ensure the link is a full URL\n",
    "        paper_page_url = f\"{BASE_URL}{paper_page_url}\"\n",
    "        \n",
    "        # Step 2: Visit each paper's Hugging Face page to extract the PDF link\n",
    "        pdf_link = get_pdf_link(paper_page_url)\n",
    "        \n",
    "        if pdf_link:\n",
    "            # Step 3: Download the PDF\n",
    "            download_pdf(pdf_link, paper_title)\n",
    "        else:\n",
    "            print(f\"PDF link not found for: {paper_title}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the page. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using My Classes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the paperscraper module\n",
    "from paperscraper import paperscraper\n",
    "\n",
    "# Create an instance of the class\n",
    "scraper = paperscraper('2024-10-04')\n",
    "\n",
    "# Use get_links function\n",
    "paper_pdfs = scraper.get_links()\n",
    "\n",
    "pdf_dict = scraper.get_pdf_text(paper_pdfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprint\n",
      "REVISIT LARGE -SCALE IMAGE -CAPTION DATA IN PRE-\n",
      "TRAINING MULTIMODAL FOUNDATION MODELS\n",
      "Zhengfeng Lai∗, Vasileios Saveris∗, Chen Chen, Hong-You Chen, Haotian Zhang\n",
      "Bowen Zhang, Juan Lao Tebar, Wenze Hu, Zhe Gan, Peter Grasch\n",
      "Meng Cao, Yinfei Yang†\n",
      "Apple AI/ML\n",
      "{jeff_lai,v_saveris,pgrasch,mengcao,yinfeiy}@apple.com\n",
      "ABSTRACT\n",
      "Recent advancements in multimodal models highlight the value of rewritten cap-\n",
      "tions for improving performance, yet key challenges remain. For example, while\n",
      "synthetic captions often provide superior quality and image-text alignment, it is\n",
      "not clear whether they can fully replace AltTexts: the role of synthetic captions\n",
      "and their interaction with original web-crawled AltTexts in pre-training is still not\n",
      "well understood. Moreover, different multimodal foundation models may have\n",
      "unique preferences for specific caption formats, but efforts to identify the optimal\n",
      "captions for each model remain limited. In this work, we propose a novel, con-\n",
      "trollable, and scalable captioning pipeline designed to generate diverse caption\n",
      "formats tailored to various multimodal models. By examining Short Synthetic\n",
      "Captions (SSC) towards Dense Synthetic Captions (DSC+) as case studies, we\n",
      "systematically explore their effects and interactions with AltTexts across models\n",
      "such as CLIP, multimodal LLMs, and diffusion models. Our findings reveal that a\n",
      "hybrid approach that keeps both synthetic captions and AltTexts can outperform the\n",
      "use of synthetic captions alone, improving both alignment and performance, with\n",
      "each model demonstrating preferences for particular caption formats. This compre-\n",
      "hensive analysis provides valuable insights into optimizing captioning strategies,\n",
      "thereby advancing the pre-training of multimodal foundation models.\n",
      "1 I NTRODUCTION\n",
      "Large-scale image-text datasets have been crucial in advancing multimodal foundation models. For\n",
      "instance, CLIP (Radford et al., 2021) is pre-trained on 400 million image-text pairs collected from the\n",
      "Web. However, web-crawled data, particularly AltText, often suffer from insufficient visual details\n",
      "and noisy content, as illustrated in Fig. 1. Recent studies highlight the benefits of synthetic captions,\n",
      "which provide better image-text alignment and improved data quality. Research on LaCLIP (Fan\n",
      "et al., 2024) and ShareGPT4V (Chen et al., 2024a) demonstrates that synthetic captions can improve\n",
      "the performance of CLIP and multimodal large language models (MLLMs), respectively. This\n",
      "raises a key question: if higher-quality synthetic captions can be generated, could they fully replace\n",
      "web-crawled AltText? Should we consider disregarding AltText altogether?\n",
      "To investigate this question, we first adopt the approach of VeCLIP (Lai et al., 2024) and train CLIP\n",
      "using synthetic captions generated by LLaV A (Liu et al., 2023b). Similar to the results discussed\n",
      "in Li et al. (2024b), training CLIP fully on synthetic captions of higher quality degrades CLIP’s\n",
      "performance significantly: as shown in Fig. 2, when compared to using only AltText, the use of\n",
      "LLaV A captions results in a substantial drop on zero-shot ImageNet classification tasks. However,\n",
      "after combining original noisy AltText and LLaV A captions, we achieve the best results in both\n",
      "classification and retrieval tasks. This observation raises a critical question: what constitutes the\n",
      "optimal image-text data for multimodal foundation models? Despite its importance, research on\n",
      "the interplay between synthetic captions and AltText remains limited. Our findings suggest that\n",
      "while rewriting techniques can enhance image-text alignment, they may reduce data diversity due to\n",
      "∗Equal contribution.\n",
      "†Corresponding author.\n",
      "1arXiv:2410.02740v1  [cs.CV]  3 Oct 2024\n",
      "Preprint\n",
      "Figure 1: The role of image-text data in multimodal foundation models: a keycomponent in training\n",
      "CLIP and Diffusion Model, and essential for multimodal LLM (MLLM) pre-training alongside text\n",
      "and interleaved image-text data. We propose a controllable captioning pipeline to synthesize different\n",
      "types of captions and explore optimal image-text data recipes for training these foundation models.\n",
      "dependence on a limited set of LLMs or MLLMs for caption generation. Specifically, since CLIP\n",
      "is a foundational vision model that benefits from learning diverse concepts, relying on synthetic\n",
      "captions can potentially hinder CLIP’s training due to lack of diversity in vocabulary and mentioned\n",
      "concepts (Fan et al., 2024).\n",
      "In addition to the role of AltText, another open question concerns the optimal formats for synthetic\n",
      "captions. For instance, advanced multimodal models like LLaV A-NeXT (Li et al., 2024a) indicate that\n",
      "recaptioned datasets are advantageous during stages focused on high-quality knowledge acquisition.\n",
      "DALL-E 3 (Betker et al., 2023) demonstrates that using 95% synthetic captions can yield superior\n",
      "results, particularly when the captions are highly descriptive. Similarly, MM1 (McKinzie et al., 2024)\n",
      "shows that even a small fraction (7%) of high-quality caption data can significantly boost few-shot\n",
      "performance. Given these insights, our work focuses on two key unresolved questions: 1) What is\n",
      "the role and value of synthetic captions, and how do they interact with the original AltText? 2) What\n",
      "types of synthetic captions are most effective for different foundation models? To address the first\n",
      "question, we revisit why prior works (Betker et al., 2023; Lai et al., 2024; McKinzie et al., 2024)\n",
      "continue using noisy web-crawled AltText, even when rewritten captions are available during training.\n",
      "Intuitively, since CLIP is a straightforward model pre-trained on image-text pairs, highly aligned\n",
      "captions should be advantageous. However, relying solely on synthetic captions may actually degrade\n",
      "CLIP’s performance, as shown in Fig. 2(a). For the second question, we investigate the effects of\n",
      "Short Synthetic Captions (SSC) and Descriptive Synthetic Captions (DSC) on CLIP. As depicted in\n",
      "Fig. 2(b), surprisingly, more descriptive captions yield inferior results compared to shorter captions\n",
      "for CLIP training, despite their greater detail.\n",
      "To explore these insights further to address the two questions, we introduce a novel, controllable, and\n",
      "scalable captioning pipeline that enables the generation of diverse caption formats at scale, tailored\n",
      "to the specific needs of different multimodal foundation models. Our pipeline is designed to build\n",
      "large-scale image-text data for the pre-training stage in a scalable way. With this pipeline, we use\n",
      "SSC and DSC as two main examples on how to customize the captioning format. Our pipeline\n",
      "can serve as a cost-effective alternative to GPT-4V for generating high-quality captions. To solve\n",
      "the second question, we conduct a comprehensive study on the effectiveness of different types of\n",
      "synthetic captions across a range of foundational models and downstream tasks. Our approach\n",
      "involves a systematic evaluation of various captioning strategies, including SSC, DSC, and mixed\n",
      "training methods that combine original AltText with synthetic data. We seek to determine the optimal\n",
      "captioning techniques for specific models, such as CLIP, diffusion models, and multimodal LLMs,\n",
      "and to assess their impact on both model performance and data diversity. Furthermore, we investigate\n",
      "the interaction between synthetic captions and original AltText, analyzing whether a hybrid approach\n",
      "can balance the need for diverse data with the benefits of enhanced image-text alignment.\n",
      "2\n",
      "Preprint\n",
      "(a)\n",
      " (b)\n",
      "Figure 2: Zero-shot retrieval and classification performance of CLIP models. (a) The effect of\n",
      "synthetic captions (LLaV A recaptioned) and AltText: solely using LLaV A captions can improve\n",
      "retrieval tasks but significantly deteriorate the zero-shot classification performance. (b) The effect of\n",
      "different formats of synthetic captions on CLIP: Short Synthetic Captions (SSC) show superior results\n",
      "to Descriptive Synthetic Captions (DSC) and the combination of them achieves the best results.\n",
      "Overall, our contributions are summarized as follows.\n",
      "•We explore the MLLM as the image describer and present a controllable and human-aligned\n",
      "captioning pipeline to convert MLLM into an image captioner.\n",
      "•We synthesize several formats of captions including Short Synthetic Captions (SSC) towards Dense\n",
      "Synthetic Captions (DSC+), then conduct extensive pre-training experiments to systematically study\n",
      "the role of synthetic captions and their intersection with original AltText across three multimodal\n",
      "foundation models.\n",
      "•We verify the image-caption training recipe that 1) AltText provide data variety and synthetic\n",
      "captions provide better image-text alignment, 2) different foundation models have their own\n",
      "preferred formats, which highlights the necessity and importance of the controllable captioning\n",
      "pipeline in building multimodal foundation models.\n",
      "2 R ELATED WORK\n",
      "Multimodal Foundation Models. CLIP (Radford et al., 2021) is one of the pioneering multimodal\n",
      "foundation models connecting images and text. By training on 400 million image-text pairs, CLIP\n",
      "shows strong zero-shot image classification and retrieval capabilities. It lays the groundwork for the\n",
      "development of more advanced multimodal foundation models, such as multimodal large language\n",
      "models (MLLMs) (Liu et al., 2023b; Wang et al., 2023; Chen et al., 2024b; Tong et al., 2024; Zhang\n",
      "et al., 2024b) for vision-language understanding and diffusion models (Rombach et al., 2022; Podell\n",
      "et al., 2023) for text-to-image generation. These advanced models often utilize CLIP’s vision tower\n",
      "as their vision encoder.\n",
      "Improving Image-Text Data. Web-crawled image-text data often suffer from issues like image-text\n",
      "misalignment and poor-quality textual descriptions (Lai et al., 2024; Li et al., 2024b). There are two\n",
      "common ways for improving image-text data: 1) data filtering based methods remove low-quality data\n",
      "such as misaligned image-text pairs by human-assisted systems (Yu et al., 2024a; Sun et al., 2023) or\n",
      "pre-trained models (Li et al., 2022b; Schuhmann et al., 2021; Gadre et al., 2024; Fang et al., 2023); 2)\n",
      "data recaptioning based methods usually leverage a LLM to rewrite the original caption or a MLLM\n",
      "to rewrite a caption for the image. For example, ShareGPT4V (Chen et al., 2024a) uses GPT-4V\n",
      "to write highly descriptive captions for their images. LaCLIP (Fan et al., 2024) leverages several\n",
      "LLMs to rewrite captions with different writing styles for data diversity. Recap-DataComp-1B (Li\n",
      "et al., 2024b) uses a LLaMA-3 based model to scale the captions. Different from the aforementioned\n",
      "works, we mainly focus on generating different types of captions and exploring 1) the format of\n",
      "ideal captions needed for each multimodal foundation model and 2) a systematic analysis of the\n",
      "intersection between AltText and synthetic captions.\n",
      "3\n",
      "Preprint\n",
      "Figure 3: Examples of controllable captions of diverse formats generated by our captioner: we can\n",
      "generate from brief to dense descriptions and fuse AltText into the caption (AFC).\n",
      "3 C USTOMIZED RE-CAPTIONING FOR MULTIMODAL FOUNDATION MODELS\n",
      "Image-text data are fundamental to the success of multimodal foundation models, serving as a bridge\n",
      "between visual and textual representations. For example, CLIP (Radford et al., 2021) is pre-trained\n",
      "on 400M web-crawled image-text pairs, enabling it to learn rich, transferable representations that can\n",
      "be applied to various downstream tasks. The importance of precise and detailed captioning is further\n",
      "highlighted in LLaV A-NeXT (Li et al., 2024a), where re-captioned detailed descriptions are utilized\n",
      "for model training at Stage-1.5, enhancing the model’s ability to understand and generate nuanced\n",
      "content. Similarly, DALL-E 3 (Betker et al., 2023) shows that the prompt-following capabilities\n",
      "of text-to-image models can be significantly improved by training on highly-descriptive generated\n",
      "image captions. This shows the critical role of captions in shaping a model’s capacity to align visual\n",
      "and textual information, ultimately improving performance across a wide range of multimodal tasks.\n",
      "However, the optimal captioning strategy for different foundational models remains under-explored.\n",
      "To address this gap, we introduce a novel, controllable, and scalable captioning pipeline designed\n",
      "to generate diverse caption formats at scale, supported by evaluation metrics that ensure high CLIP\n",
      "4\n",
      "Preprint\n",
      "Figure 4: Directly using MLLMs as image captioners may result in hallucinations and gener-\n",
      "ate captions that do not align with specific instructions: both LLaV A (Liu et al., 2023b) and\n",
      "ShareGPT4V (Chen et al., 2024a) generate over three sentences and obvious hallucination.\n",
      "scores and minimal hallucination. We summarize the capability of our captioning model by generating\n",
      "the following formats of captions as shown in Fig. 3:\n",
      "•Short Synthetic Caption (SSC) : a concise sentence that describes the primary subject of the\n",
      "image.\n",
      "•Descriptive Synthetic Caption (DSC) : a description limited to 78 tokens, emphasizing the central\n",
      "subject and key visual elements.\n",
      "•Dense Synthetic Caption (DSC+) : a more comprehensive description detailing the main subject\n",
      "along with the background, setting, and any significant objects or actions.\n",
      "•AltText Fusion Caption (AFC) : a caption similar to DSC, but integrated with AltText where\n",
      "appropriate. This type of caption removes unnecessary details often found in AltText, offering a\n",
      "cleaner, more cohesive description than a simple concatenation of AltText and synthetic caption.\n",
      "3.1 MLLM ASANIMAGE DESCRIBER\n",
      "VeCLIP (Lai et al., 2024) employs LLaV A (Liu et al., 2023b) for image captioning, while\n",
      "ShareGPT4V (Chen et al., 2024a) utilizes GPT-4V for this task. Compared to traditional image\n",
      "captioners like BLIP (Li et al., 2022a), MLLMs offer several advantages and are natually good\n",
      "image describers. MLLMs can generate longer and more detailed captions, as demonstrated by\n",
      "LLaV A-NeXT (Li et al., 2024a), where a 34B model was used to produce highly descriptive cap-\n",
      "tions. This ability to generate more context-aware descriptions stems from the integration of a\n",
      "large language model (LLM) with a vision encoder. This combination allows MLLMs to capture\n",
      "fine-grained visual details and complex inter-object relationships. Additionally, MLLMs benefit from\n",
      "their multi-stage training process, which combines pre-training on large-scale datasets and supervised\n",
      "fine-tuning for downstream tasks. These characteristics make MLLMs a powerful tool for generating\n",
      "more descriptive captions. However, directly using MLLM as an image describer may have two\n",
      "major limitations: 1) MLLM may not strictly follow the instruction to generate a specific format of\n",
      "caption (Liu et al., 2023a); 2) these instruction fine-tuned MLLMs tend to hallucinate. As shown\n",
      "in Fig. 4, both LLaV A (Liu et al., 2023b) and ShareGPT4V (Chen et al., 2024a) fail to describe the\n",
      "image using only three sentences but generate hallucinated contents (highlighted in red). Although\n",
      "GPT-4V shows stronger capability and many works use it for captioning, the scalability remains\n",
      "limited due to the cost. Therefore, in this work, we focus on building a cost-effective captioner\n",
      "instead of using GPT-4V .\n",
      "To alleviate the above two limitations, we first investigate the origins of hallucination in MLLMs,\n",
      "hypothesizing two primary sources: 1) inherent limitations of the LLM, and 2) the quality of\n",
      "supervised fine-tuning (SFT) datasets, which are often themselves synthetically derived or processed.\n",
      "We focus on the latter, proposing that mitigating hallucinations at the dataset level is essential for\n",
      "converting an MLLM into an effective captioner. We also address the format-following issue by\n",
      "fine-tuning the MLLM on a curated captioning-specific dataset, transforming it into a purpose-built\n",
      "captioning model to generate diverse captions.\n",
      "3.2 T WO-STAGE HUMAN -ALIGNED CAPTIONING\n",
      "Stage 1: transforming MLLM into a customized captioner. To minimize hallucinations from\n",
      "MLLMs, we begin by constructing a clean and precise fine-tuning dataset. Instead of relying on\n",
      "5\n",
      "Preprint\n",
      "Figure 5: Overview of the controllable and human-aligned captioning pipeline. In Stage 1, we convert\n",
      "a pre-trained MLLM into a customized captioner that strictly follows the captioning instructions. In\n",
      "Stage 2, we leverage human-aligned captions to further fine-tune the captioner.\n",
      "GPT-4 generated data, we curate a high-quality dataset of human-annotated image-text pairs, named\n",
      "Stage-1-1M. This dataset contains short, human-curated captions, with five captions per image.\n",
      "Additionally, we integrate an OCR detection model to extract in-image text, which, alongside the\n",
      "captions, is fed into an LLM for summarization and controlled rewriting. By employing a strict\n",
      "prompt, we prevent the LLM from introducing extraneous information, while few-shot prompts guide\n",
      "the model in generating various caption formats, including both concise and descriptive styles. We\n",
      "further enhance the dataset through post-processing, using heuristic and model-based quality checks\n",
      "to improve its overall quality. This comprehensive fine-tuning on the Stage-1-1M dataset transforms\n",
      "the MLLM, specifically the 3B version of MM1 (McKinzie et al., 2024), into a customized captioner\n",
      "that aligns closely with the intended output characteristics.\n",
      "Stage 2: Human-aligned further fine-tuning. While the Stage-1-1M dataset effectively establishes\n",
      "a foundation, it lacks the depth required for more descriptive captioning tasks. In Stage 2, we address\n",
      "this by incorporating descriptive human-annotated data to enhance caption diversity and quality. We\n",
      "curate a new dataset, named Stage-2-HA, specifically designed for detailed captioning. This dataset\n",
      "is meticulously annotated by human experts to capture nuanced visual elements and complex scene\n",
      "descriptions. Post-annotation, we leverage an LLM to reformat these captions into multiple stylistic\n",
      "variations, including both concise and richly descriptive formats. By imposing strict constraints\n",
      "during the LLM processing, we ensure alignment with human-generated content, avoiding the pitfalls\n",
      "of hallucination. The captioner is then fine-tuned on the Stage-2-HA dataset, resulting in a highly\n",
      "refined and human-aligned captioning model capable of generating captions tailored to specific use\n",
      "cases. This dual-stage fine-tuning process not only enhances the model’s adaptability but also ensures\n",
      "a balance between brevity and descriptiveness, making it a versatile tool in controllable captioning.\n",
      "The overview of the complete captioning pipeline is shown in Fig. 5.\n",
      "3.3 C APTION ANALYSIS\n",
      "Richness assessment: token length and average number of assertions. Fig. 6 illustrates the\n",
      "distribution of the number of tokens of various caption types generated in this study. Specifically,\n",
      "SSC mainly ranges from 10 to 15 tokens, while DSC spans from 40 to 60 tokens, both fitting within\n",
      "the text encoder’s capacity in CLIP. In contrast, most DSC+ captions exceed 100 tokens. Besides that,\n",
      "we propose Average Number of Assertions (ANA) to quantify the richness of captions. We prompt\n",
      "an LLM to generate different assertions of a caption to analyze our different formats of captions in\n",
      "terms of the richness. More details of this approach is in Appendix. Note that the ANA for SSC is\n",
      "2.49, DSC as 8.13 and DSC+ as 12.20, showing more visual contents.\n",
      "Diversity assessment: number of unique entities in captions. We hypothesize that the original,\n",
      "albeit noisy, AltText may carry a broader range of diverse information and knowledge, offering\n",
      "potential advantages for CLIP’s pre-training. To assess this diversity, we quantify the number of\n",
      "6\n",
      "Preprint\n",
      "Figure 6: Distribution of token lengths of our generated captions in four formats: we caption COCO\n",
      "2017 images and visualize their distributions.\n",
      "Figure 7: The number of unique entities in different synthetic captions (randomly sample 17.5k\n",
      "images) compared to AltText: AltText provides more unique entities as wider knowledge.\n",
      "unique entities present in the captions and present a visualization in Fig. 7. Our analysis shows that\n",
      "AltText contains a higher number of unique entities, which could be beneficial in providing a wider\n",
      "knowledge base.\n",
      "4 I MAGE -CAPTION DATA FOR MULTIMODAL FOUNDATION MODELS\n",
      "In this section, we mainly discuss three foundation models: CLIP, multimodal LLM, and diffusion\n",
      "models. For both CLIP and diffusion models, since the text encoder is limited to 77 tokens (Zhang\n",
      "et al., 2024a), we focus primarily on SSC and DSC. For multimodal LLM, we explore more detailed\n",
      "versions, including DSC+ and AFC. We summarize our key findings below:\n",
      "•The tradeoff between the richness of captions and their accuracy needs to be balanced based on the\n",
      "multimodal tasks.\n",
      "•Both AltText and synthetic captions are important for CLIP training, with shorter captions yielding\n",
      "better performance. Linear probing is an additional effective way to evaluate the representations.\n",
      "•Pre-training and SFT benchmarks can behave differently in multimodal LLMs. On the SFT\n",
      "benchmarks, MM1 shows a preference for DSC+ alone.\n",
      "• For diffusion models, DSC emerges as the most effective captioning strategy.\n",
      "7\n",
      "Preprint\n",
      "Table 1: Effect of different synthetic captions on CLIP with\n",
      "ViT-B/16 as the backbone. IN: ImageNet.\n",
      "Pre-train CaptionCOCO (R@1) Flickr30k (R@1)IN INV2I-T T-I I-T T-I\n",
      "AltText 54.24 36.98 81.30 65.80 65.70 58.58\n",
      "DSC 52.28 29.00 80.90 54.75 27.30 21.91\n",
      "SSC 57.00 35.15 84.67 63.35 49.10 43.31\n",
      "AFC 54.82 34.84 84.00 62.18 38.98 35.11\n",
      "DSC + AltText 65.84 46.08 90.26 73.94 66.18 58.74\n",
      "SSC + AltText 66.67 48.13 91.81 76.54 66.63 59.57\n",
      "AFC + AltText 63.98 43.76 89.10 73.32 66.47 58.84\n",
      "All Synthetic + AltText 70.12 50.21 93.00 77.72 64.91 57.92Table 2: Evaluation with linear probing (LP)\n",
      "on ImageNet for CLIP.\n",
      "Pre-train Caption Zero-shot LP Gain\n",
      "AltText 65.70 78.34 +12.64\n",
      "DSC 27.30 75.72 +48.42\n",
      "SSC 66.63 79.94 +13.31\n",
      "AFC 38.98 75.53 +36.55\n",
      "DSC + AltText 66.18 79.96 +13.78\n",
      "SSC + AltText 66.63 79.94 +13.31\n",
      "AFC + AltText 66.47 78.70 +12.23\n",
      "DSC + SSC + AltText 65.16 80.01 +14.85\n",
      "All Synthetic + AltText 64.91 79.55 +14.64\n",
      "4.1 I MAGE -CAPTION DATA FOR CLIP\n",
      "We use VeCap-300M (Lai et al., 2024), a web-crawled dataset with raw AltText as our main pre-\n",
      "training dataset for CLIP. Besides AltText, we generate several synthetic caption datasets for the\n",
      "study. Then, we use ViT-B/16 as the vision encoder. The training details can be found in Appendix.\n",
      "Effect of synthetic captions. We first study the effect of Short Synthetic Captions (SSC) and\n",
      "Descriptive Synthetic Captions (DSC). Results are summarized in Table 1. Interestingly, while\n",
      "SSC enhances retrieval performance, it leads to a substantial drop in zero-shot ImageNet accuracy.\n",
      "Moreover, despite demonstrating that synthetic captions have superior image-text alignment and\n",
      "reduced noise, the more descriptive captions (DSC) perform worse across all benchmarks. Based\n",
      "on these results, DSC appears suboptimal for CLIP training, leading to inferior performance. For\n",
      "example, despite DSC capturing more visual concepts within the captions, its zero-shot performance\n",
      "shows a significant degradation of 21.8% compared to SSC. We hypothesize this performance drop\n",
      "may be partially due to a distribution mismatch as the prompts in COCO/Flickr30k/ImageNet datasets\n",
      "are short (e.g., “a photo of {}”).\n",
      "To further explore the performance gap between DSC and SSC on CLIP, we also use linear probing,\n",
      "which provides a direct measure of the quality and generalization capability of the representations\n",
      "learned by CLIP. Strong performance from a linear classifier on specific tasks indicates that the\n",
      "pre-trained model has effectively captured relevant and discriminative features, underscoring the\n",
      "robustness of its embeddings. We summarize the results on linear probing in Table 2. Even though\n",
      "DSC and SSC show lower zero-shot performance, they achieve comparable results to AltText after\n",
      "linear probing, indicating similar pre-trained representations. This indicates that the relatively\n",
      "poor zero-shot results of DSC can be significantly improved with linear probing, implying that\n",
      "representations learned from DSC are richer than initially presumed.\n",
      "Intersection of synthetic captions and AltText. From Table 1, we find training CLIP solely on\n",
      "large-scale synthetic captions may get inferior results compared to the original AltText, even though\n",
      "synthetic captions have better image-text alignment. We hypothesize that synthetic captions rewritten\n",
      "from a MLLM may hurt the diversity and knowledge coverage of the original AltText. Therefore,\n",
      "we blend our synthetic captions and AltText, such as DSC + AltText and SSC + AltText: there is\n",
      "a significant boost in retrieval performance—e.g., over 10% improvement on COCO. Additionally,\n",
      "SSC + AltText also leads to gains in ImageNet accuracy. Utilizing a mixture of all synthetic caption\n",
      "formats yields the best performance in retrieval tasks, likely due to the wider entity-based knowledge,\n",
      "as visualized in Fig. 7.\n",
      "Optimal mixture ratio between synthetic captions and AltText. Considering the wider knowledge\n",
      "of AltText (Fig. 7) and the better alignment of synthetic captions, we explore the optimal mixture ratio.\n",
      "Specficially, we use SSC and AltText from VeCap-300M (Lai et al., 2024) as an example, with results\n",
      "shown in Fig. 8. A ratio of 0 corresponds to using only SSC, while 100 corresponds to using only\n",
      "AltText. We observe that CLIP achieves optimal performance across both retrieval and classification\n",
      "tasks when the ratio is tuned to around 40-50%. A lower proportion of AltText leads to a drop in\n",
      "retrieval performance, whereas a lower proportion of SSC results in decreased accuracy in ImageNet\n",
      "zero-shot classification. This observation aligns with findings in Li et al. (2024b). From this, we\n",
      "verify that AltText provides broader knowledge coverage and greater diversity, which benefits CLIP’s\n",
      "pre-training by enabling the model to grasp a wider range of concepts. This diversity may serve as\n",
      "a foundation for generalization, allowing CLIP to better represent varied contexts and domains in\n",
      "zero-shot classification.\n",
      "8\n",
      "Preprint\n",
      "Figure 8: The intersection of synthetic captions and AltText for CLIP. We gradually increase the\n",
      "proportion of SSC mixed with AltText during training. All experiments use ViT-B/16 as the backbone\n",
      "and VeCap-300M (Lai et al., 2024) as the pre-training dataset.\n",
      "Exploration of the optimal way of using AltText: AFC vs. simple mixture. As shown in Fig. 8\n",
      "and Table 1, we verify that a simple mixture of our synthetic captions and AltText can achieve\n",
      "superior results. In addition to this straightforward combination, we investigate alternative methods\n",
      "of incorporating AltText into the training process. One promising approach is to fuse the knowledge\n",
      "from AltText directly into the synthetic captions. To enable this, we fine-tune our captioner with\n",
      "instruction-following capabilities, generating AltText Fusion Captions (AFC). While AFC shows\n",
      "improvement over DSC due to the enriched AltText information, it underperforms when compared to\n",
      "the simple mixture of DSC and AltText.\n",
      "4.2 I MAGE -CAPTION DATA FOR MULTIMODAL LLM\n",
      "Table 3: The effect of synthetic captions on MM1\n",
      "(1.2B model) pre-training.\n",
      "Pre-train Caption TextCore 0-Shot 4-Shot 8-Shot\n",
      "AltText 53.48 34.80 55.81 59.72\n",
      "DSC 53.76 37.05 59.36 63.68\n",
      "DSC + AltText 53.71 37.09 60.31 63.96\n",
      "SSC 53.46 37.35 59.19 63.56\n",
      "SSC + AltText 53.20 37.16 58.22 62.22Table 4: The ratio ablation on MM1 pre-training\n",
      "between synthetic captions and AltText.\n",
      "Mixing Ratio TextCore 0-Shot 4-Shot 8-Shot\n",
      "33/66 53.86 35.16 59.69 63.97\n",
      "50/50 53.86 36.40 60.17 64.13\n",
      "66/33 54.24 37.02 60.26 64.71\n",
      "80/20 53.96 35.74 60.09 64.69\n",
      "100/0 54.19 15.54 60.24 64.02\n",
      "As we study large-scale image-caption data for MLLMs, we use MM1 (McKinzie et al., 2024) as one\n",
      "example and focus on the pre-training stage. MM1 (McKinzie et al., 2024) claims that captioning\n",
      "data lift the zero-shot performance and synthetic captions are helpful for few-shot learning. Based\n",
      "on this insight, we further study the captioning data recipe on how to balance the use of original\n",
      "AltText and synthetic captions. We follow the pre-training setup and the evaluation benchmark in\n",
      "MM1 (McKinzie et al., 2024) to report TextCore and 0/4/8-shot performance. All of the experiments\n",
      "are conducted on the 1.2B model. We pre-train the model with 50K steps and the batch size is 512.\n",
      "More details are in Appendix.\n",
      "Effect of synthetic captions for pre-training benchmarks. We generate DSC and SSC captions for\n",
      "VeCap-300M (Lai et al., 2024), as used in MM1 (McKinzie et al., 2024), and replace the captions\n",
      "during MM1 pre-training. The results, summarized in Table 3, show that our synthetic captions yield\n",
      "improved performance in image-text benchmarks across 0-shot to 8-shot settings. For example, SSC\n",
      "achieves a +1.3% performance gain in 0-shot evaluation compared to the original MM1. Unlike CLIP\n",
      "experiments, DSC outperforms SSC, with the combination of DSC and original AltText delivering\n",
      "the best results in this context. We also conduct an ablation study of data mixing ratios on MM1\n",
      "pre-training to explore the optimal balance between synthetic captions and AltText, as summarized in\n",
      "Table 4. The results indicate that a 66/33 mixing ratio yields the best overall performance across all\n",
      "9\n",
      "Preprint\n",
      "Table 5: SFT evaluation of models pre-trained with different types of captions. We use the same SFT evaluation\n",
      "benchmarks as in MM1 (McKinzie et al., 2024). (*): Concatenation of two captions.\n",
      "Pre-Trained Data VQAv2 VQATMMMU MathV MMEPMMECSEED POPE LLaV AWAverage\n",
      "AltText 77.1 66.1 31.4 28.3 781.7 225.4 61.7 84.6 69.8 57.6\n",
      "LLaV A Caption 78.0 65.4 30.0 27.6 773.4 200.4 63.5 83.7 63.6 56.5\n",
      "SSC 78.8 65.8 33.6 28.2 760.2 216.4 63.1 84.4 69.0 57.9\n",
      "DSC 77.1 61.8 30.8 27.9 596.6 213.6 64.6 83.9 70.8 56.3\n",
      "DSC+ 79.0 65.4 32.6 29.4 727.2 224.3 66.8 85.1 71.5 58.7\n",
      "AltText + SSC (*) 77.7 65.4 32.3 27.6 781.9 236.1 62.0 84.2 70.3 57.7\n",
      "AltText + DSC (*) 78.2 66.9 31.9 30.7 686.4 216.4 63.8 84.3 72.0 58.2\n",
      "AltText + DSC+ (*) 79.2 66.9 31.1 29.6 740.2 229.3 65.5 85.1 68.1 58.2\n",
      "AFC 78.0 65.6 32.3 29.4 713.2 214.3 66.0 84.4 70.0 58.0\n",
      "SSC + DSC + DSC+ 79.4 66.5 30.2 30.3 689.3 198.9 65.5 83.8 67.5 57.5\n",
      "AFC + SSC + DSC + DSC+ 77.3 62.6 31.6 27.7 661.5 198.2 64.1 83.5 64.2 55.9\n",
      "evaluation settings. Specifically, this ratio achieves the highest scores for TextCore (54.24), 0-shot\n",
      "(37.02), 4-shot (60.26), and 8-shot (64.71) performance. While increasing the proportion of synthetic\n",
      "captions generally improves performance, there is a significant drop in 0-shot performance when\n",
      "using only synthetic captions (100/0 ratio).\n",
      "Effect of synthetic captions for SFT benchmarks. Besides pre-training benchmarks, we also\n",
      "conduct SFT and then evaluate the model to analyze the profound effect of image-caption data in\n",
      "MLLMs. We use the same SFT recipe to have a fair comparison. As shown in Table 5, DSC+\n",
      "and the concatenation of AltText with DSC+ deliver the best performance. This strongly suggests\n",
      "the importance of detailed captions, despite them containing potentially the highest number of\n",
      "hallucinations among all the caption types we test. On the other hand, concatenating AltText with\n",
      "synthetic captions does not yield significant improvement in the SFT benchmarks, contrasting with\n",
      "the gains observed in pre-training benchmarks. We hypothesize that the primary role of image-caption\n",
      "data during the pre-training phase of MLLMs is to enhance image-text alignment. Consequently,\n",
      "more detailed captions, such as DSC+, deliver superior results after the SFT stage.\n",
      "DSC+ alone can outperform diverse synthetic captions. Unlike CLIP, where mixing diverse\n",
      "synthetic captions leads to superior results, for MM1, detailed captions (DSC+) alone yield the best\n",
      "performance after the SFT stage. As shown in Table 5, DSC+ achieves a 58.7% score, outperforming\n",
      "LLaV A captions (56.5%) by 2.2%. This suggests that providing richer and more specific information\n",
      "in captions helps multimodal LLMs like MM1 generalize better after the SFT stage. The combination\n",
      "of SSD, DSC, DSC+, and AFC does not lead to better results, suggesting that multimodal LLMs may\n",
      "benefit more from detailed captions during pre-training. These findings suggest that while combining\n",
      "synthetic captions proves beneficial in some contexts (e.g., CLIP), for multimodal LLMs like MM1,\n",
      "a single, detailed caption offers more effective guidance during pre-training.\n",
      "4.3 I MAGE -CAPTION DATA FOR DIFFUSION MODEL\n",
      "Inspired by DALLE-3 (Betker et al., 2023), detailed and short captions can improve the prompt\n",
      "following ability. In this work, our DSC not only covers the main objects within the scene, but also\n",
      "their relationships, attributes, and the broader context in which they are situated. We hypothesize that\n",
      "this level of detail allows the model to generate images that are not only visually accurate but also\n",
      "semantically aligned with the textual input. We implement Stable Diffusion 3 (Esser et al., 2024) and\n",
      "use this diffusion model as our studying example on text-to-image generation tasks. The backbone is\n",
      "based on the DiT architecture (Peebles & Xie, 2023) that focuses exclusively on class-conditional\n",
      "image generation and incorporates a modulation mechanism to condition the network based on\n",
      "both the diffusion process timestep and the class label. Different from DALLE-3 (Betker et al.,\n",
      "2023), we report results on more comprehensive benchmarks instead of only CLIP score, such as\n",
      "GenEval (Ghosh et al., 2024) and DSG (Cho et al., 2024).\n",
      "Effect of synthetic captions. Synthetic captions lead to significant improvements on the GenEval\n",
      "benchmark (Ghosh et al., 2024), as shown in Table 6, highlighting the advantage of enhanced\n",
      "prompt-following capabilities. Notably, incorporating SSC or DSC with AltText boosts the GenEval\n",
      "average score from 58.8 to 65.5. Additionally, synthetic captions yield over a 3.5% improvement\n",
      "on the DSG benchmark (Cho et al., 2024). However, SSC achieves better performance on the FID\n",
      "score (Jayasumana et al., 2024). Overall, the descriptive captions show better results among these\n",
      "benchmarks.\n",
      "10\n",
      "Preprint\n",
      "Table 6: The effect of synthetic captions on diffusion models.\n",
      "GenEval (Ghosh et al., 2024) FID\n",
      "COCO30kDSG\n",
      "Average Single Obj Two Obj Counting Colors Position Attribution Average\n",
      "AltText 99.1 80.4 58.3 78.5 17.4 40.2 62.3 13.6 72.4\n",
      "SSC 98.5 80.1 68.9 80.4 18.6 50.2 66.1 13.1 73.1\n",
      "SSC + AltText 98.2 84.4 65.1 77.3 18.6 50.2 65.5 13.1 74.2\n",
      "DSC 93.9 60.2 67.7 79.8 13.8 39.2 59.1 14.8 73.8\n",
      "DSC + AltText 99.2 84.2 68.9 79.0 21.4 52.0 67.4 14.0 74.2\n",
      "Figure 9: The intersection of synthetic captions and AltText for diffusion models. We gradually\n",
      "increase the proportion of DSC mixed with AltText during training.\n",
      "Ablation study on mixing ratio of synthetic captions and AltText. We examine the impact of\n",
      "varying the ratio between DSC and AltText in diffusion model training, evaluating performance across\n",
      "FID@COCO30k (Jayasumana et al., 2024), CLIP@COCO30k, DSG Average (Cho et al., 2024),\n",
      "GenEval Overall (Ghosh et al., 2024). Results are summarized in Fig. 9. The FID@COCO30k metric\n",
      "shows a gradual increase, suggesting that higher DSC ratios lead to improvements in generation\n",
      "quality. The DSG Average score exhibits improvements with a higher DSC ratio, indicating that DSC\n",
      "can enhance the model’s ability to handle complex tasks. However, the performance on the GenEval\n",
      "related metric peaks at 50% DSC, after which it begins to decline, highlighting the necessity of\n",
      "balancing synthetic and original captions to achieve optimal results across diverse evaluation merics.\n",
      "Besides DSC, we further conduct experiments using SSC as well. Results are summarized in Table\n",
      "6. Overall, the use of SSC alsone also achieves competitive performance, but the use of DSC and\n",
      "AltText together appears to be a better captioning strategy.\n",
      "5 D ISCUSSION\n",
      "In this study, we examine the role and value of image-text data in multimodal foundation models,\n",
      "including CLIP, multimodal LLMs, and diffusion models. Our research focuses on the intersection\n",
      "between synthetic image-aligned captions and the original web-crawled AltText. To identify the most\n",
      "effective captions for each foundation model, we develop a controllable and human-aligned captioning\n",
      "pipeline designed to minimize hallucinations and generate various types of captions as needed.\n",
      "Through extensive pre-training experiments, we derive the following key insights. 1) Both AltText and\n",
      "synthetic captions play crucial roles—AltText contributes to more diverse information, while synthetic\n",
      "captions offer improved image-text alignment. 2) CLIP tends to favor short synthetic captions,\n",
      "whereas MLLMs benefit from more descriptive captions. We also observe that the benchmarks in\n",
      "the pre-training and SFT stage of MLLMs may have different preferences of captions. 3) We verify\n",
      "the observation from DALLE-3 on text-to-image generation with more comprehensive benchmarks\n",
      "and show the benefits of synthetic captions. For future work, we aim to further refine our captioning\n",
      "pipeline, enhancing its ability to generate task-specific captions across a wider range of multimodal\n",
      "applications.\n",
      "11\n",
      "Preprint\n",
      "REFERENCES\n",
      "Jonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\n",
      "question-answer pairs. In Proceedings of the 2013 conference on empirical methods in natural\n",
      "language processing , pp. 1533–1544, 2013.\n",
      "James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang\n",
      "Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer\n",
      "Science. https://cdn. openai. com/papers/dall-e-3. pdf , 2(3):8, 2023.\n",
      "Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical\n",
      "commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence ,\n",
      "volume 34, pp. 7432–7439, 2020.\n",
      "James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal\n",
      "Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao\n",
      "Zhang. JAX: composable transformations of Python+NumPy programs. Github , 2018. URL\n",
      "http://github.com/google/jax .\n",
      "Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing\n",
      "multimodal llm’s referential dialogue magic. arXiv preprint arXiv:2306.15195 , 2023.\n",
      "Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin.\n",
      "Sharegpt4v: Improving large multi-modal models with better captions. ECCV , 2024a.\n",
      "Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong\n",
      "Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning\n",
      "for generic visual-linguistic tasks. In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition , pp. 24185–24198, 2024b.\n",
      "Jaemin Cho, Yushi Hu, Jason Baldridge, Roopal Garg, Peter Anderson, Ranjay Krishna, Mohit Bansal,\n",
      "Jordi Pont-Tuset, and Su Wang. Davidsonian scene graph: Improving reliability in fine-grained\n",
      "evaluation for text-to-image generation. In ICLR , 2024.\n",
      "Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and\n",
      "Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge.\n",
      "arXiv preprint arXiv:1803.05457 , 2018.\n",
      "Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\n",
      "Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\n",
      "models with instruction tuning, 2023.\n",
      "Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam\n",
      "Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for\n",
      "high-resolution image synthesis. In Forty-first International Conference on Machine Learning ,\n",
      "2024.\n",
      "Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training\n",
      "with language rewrites. Advances in Neural Information Processing Systems , 36, 2024.\n",
      "Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander T Toshev, and Vaishaal\n",
      "Shankar. Data filtering networks. In The Twelfth International Conference on Learning Represen-\n",
      "tations , 2023.\n",
      "Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu\n",
      "Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive evaluation\n",
      "benchmark for multimodal large language models, 2024.\n",
      "Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen,\n",
      "Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the\n",
      "next generation of multimodal datasets. Advances in Neural Information Processing Systems , 36,\n",
      "2024.\n",
      "12\n",
      "Preprint\n",
      "Dhruba Ghosh, Hannaneh Hajishirzi, and Ludwig Schmidt. Geneval: An object-focused framework\n",
      "for evaluating text-to-image alignment. Advances in Neural Information Processing Systems , 36,\n",
      "2024.\n",
      "Drew A Hudson and Christopher D Manning. Gqa: A new dataset for real-world visual reasoning\n",
      "and compositional question answering. In Proceedings of the IEEE/CVF conference on computer\n",
      "vision and pattern recognition , pp. 6700–6709, 2019.\n",
      "Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and\n",
      "Sanjiv Kumar. Rethinking fid: Towards a better evaluation metric for image generation. In\n",
      "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition , pp.\n",
      "9307–9315, 2024.\n",
      "Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\n",
      "supervised challenge dataset for reading comprehension. arXiv preprint arXiv:1705.03551 , 2017.\n",
      "Zhengfeng Lai, Haotian Zhang, Bowen Zhang, Wentao Wu, Haoping Bai, Aleksei Timofeev, Xianzhi\n",
      "Du, Zhe Gan, Jiulong Shan, Chen-Nee Chuah, Yinfei Yang, and Meng Cao. Veclip: Improving clip\n",
      "training via visual-enriched captions, 2024. URL https://arxiv.org/abs/2310.07699 .\n",
      "Bo Li, Hao Zhang, Kaichen Zhang, Dong Guo, Yuanhan Zhang, Renrui Zhang, Feng\n",
      "Li, Ziwei Liu, and Chunyuan Li. Llava-next: What else influences visual instruc-\n",
      "tion tuning beyond data?, May 2024a. URL https://llava-vl.github.io/blog/\n",
      "2024-05-25-llava-next-ablations/ .\n",
      "Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Bench-\n",
      "marking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125 ,\n",
      "2023a.\n",
      "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\n",
      "training for unified vision-language understanding and generation. In International conference on\n",
      "machine learning , pp. 12888–12900. PMLR, 2022a.\n",
      "Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-\n",
      "training for unified vision-language understanding and generation. In International conference on\n",
      "machine learning , pp. 12888–12900. PMLR, 2022b.\n",
      "Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru\n",
      "Mei, Qing Liu, Huangjie Zheng, et al. What if we recaption billions of web images with llama-3?\n",
      "arXiv preprint arXiv:2406.08478 , 2024b.\n",
      "Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. Evaluating object\n",
      "hallucination in large vision-language models. In EMNLP , 2023b.\n",
      "Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating\n",
      "hallucination in large multi-modal models via robust instruction tuning. In The Twelfth International\n",
      "Conference on Learning Representations , 2023a.\n",
      "Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023b.\n",
      "Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord,\n",
      "Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for\n",
      "science question answering. Advances in Neural Information Processing Systems , 35:2507–2521,\n",
      "2022.\n",
      "Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng,\n",
      "Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning\n",
      "of foundation models in visual contexts. arXiv preprint arXiv:2310.02255 , 2023.\n",
      "Brandon McKinzie, Zhe Gan, Jean-Philippe Fauconnier, Sam Dodge, Bowen Zhang, Philipp Dufter,\n",
      "Dhruti Shah, Xianzhi Du, Futang Peng, Floris Weers, et al. Mm1: Methods, analysis & insights\n",
      "from multimodal llm pre-training. arXiv preprint arXiv:2403.09611 , 2024.\n",
      "13\n",
      "Preprint\n",
      "Denis Paperno, Germán Kruszewski, Angeliki Lazaridou, Quan Ngoc Pham, Raffaella Bernardi,\n",
      "Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernández. The lambada dataset:\n",
      "Word prediction requiring a broad discourse context. arXiv preprint arXiv:1606.06031 , 2016.\n",
      "William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of\n",
      "the IEEE/CVF International Conference on Computer Vision , pp. 4195–4205, 2023.\n",
      "Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe\n",
      "Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image\n",
      "synthesis. arXiv preprint arXiv:2307.01952 , 2023.\n",
      "Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,\n",
      "Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual\n",
      "models from natural language supervision. In ICML , pp. 8748–8763, 2021.\n",
      "Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\n",
      "Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text\n",
      "transformer. Journal of machine learning research , 21(140):1–67, 2020.\n",
      "Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object\n",
      "hallucination in image captioning. EMNLP , 2018.\n",
      "Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-\n",
      "resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\n",
      "ence on computer vision and pattern recognition , pp. 10684–10695, 2022.\n",
      "Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An\n",
      "adversarial winograd schema challenge at scale. Communications of the ACM , 64(9):99–106,\n",
      "2021.\n",
      "Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis,\n",
      "Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of\n",
      "clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114 , 2021.\n",
      "Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\n",
      "and Marcus Rohrbach. Towards vqa models that can read. In CVPR , 2019a.\n",
      "Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and\n",
      "Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference\n",
      "on computer vision and pattern recognition , pp. 8317–8326, 2019b.\n",
      "Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan,\n",
      "Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with\n",
      "factually augmented rlhf. arXiv preprint arXiv:2309.14525 , 2023.\n",
      "Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha\n",
      "Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et al. Cambrian-1: A fully open,\n",
      "vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860 , 2024.\n",
      "Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang,\n",
      "Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang.\n",
      "Cogvlm: Visual expert for pretrained language models, 2023.\n",
      "Johannes Welbl, Nelson F Liu, and Matt Gardner. Crowdsourcing multiple choice science questions.\n",
      "arXiv preprint arXiv:1707.06209 , 2017.\n",
      "Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu,\n",
      "Hai-Tao Zheng, Maosong Sun, et al. Rlhf-v: Towards trustworthy mllms via behavior alignment\n",
      "from fine-grained correctional human feedback. In Proceedings of the IEEE/CVF Conference on\n",
      "Computer Vision and Pattern Recognition , pp. 13807–13816, 2024a.\n",
      "Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,\n",
      "and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. In\n",
      "Forty-first International Conference on Machine Learning , 2024b.\n",
      "14\n",
      "Preprint\n",
      "Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,\n",
      "Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multimodal\n",
      "understanding and reasoning benchmark for expert agi. arXiv preprint arXiv:2311.16502 , 2023.\n",
      "Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine\n",
      "really finish your sentence? arXiv preprint arXiv:1905.07830 , 2019.\n",
      "Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov,\n",
      "and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the\n",
      "IEEE/CVF conference on computer vision and pattern recognition , pp. 18123–18133, 2022.\n",
      "Beichen Zhang, Pan Zhang, Xiaoyi Dong, Yuhang Zang, and Jiaqi Wang. Long-clip: Unlocking the\n",
      "long-text capability of clip. ECCV , 2024a.\n",
      "Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah,\n",
      "Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev,\n",
      "Mingze Xu, Hong-You Chen, Jean-Philippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang,\n",
      "Afshin Dehghan, Peter Grasch, and Yinfei Yang. Mm1.5: Methods, analysis & insights from\n",
      "multimodal llm fine-tuning. arXiv preprint arXiv:2409.20566 , 2024b.\n",
      "Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: En-\n",
      "hancing vision-language understanding with advanced large language models. arXiv preprint\n",
      "arXiv:2304.10592 , 2023.\n",
      "15\n",
      "Preprint\n",
      "Appendices\n",
      "We provide additional details for datasets, experimental settings, results, and analysis in the supple-\n",
      "mentary material.\n",
      "A E XPERIMENTAL DETAILS\n",
      "A.1 CLIP\n",
      "We summarize the training details in Table A1. For the pre-training stage, we pre-train models on up\n",
      "to 512 TPUs with JAX (Bradbury et al., 2018).\n",
      "Table A1: Pre-training hyper-parameters and settings for the in-house CLIP.\n",
      "Batch size 32768\n",
      "Image size 224×224(ViT-B/16)\n",
      "Image pre-processing long-side resizing with padding (i.e., tf.image.resize_with_pad )\n",
      "Text tokenizer T5 (Raffel et al., 2020), lowercase\n",
      "Text maximum length 77\n",
      "Steps 435,000(i.e.,∼14B examples seen)\n",
      "Optimizer AdamW ( β1= 0.9, β2= 0.98)\n",
      "Peak learning rate (LR) 0.0005\n",
      "LR schedule cosine decays with linear warm-up (first 2k steps)\n",
      "Weight decay 0.2\n",
      "Dropout rate 0.0\n",
      "A.1.1 A DDITIONAL EXPERIMENTS\n",
      "To further explore the performance gap between DSC and SSC on CLIP, we present two additional\n",
      "benchmarks to enhance the representativeness of CLIP’s existing evaluations: 1) linear probing and\n",
      "2) transferability between CLIP pre-trained with different captions and LLaV A-style MLLMs. Linear\n",
      "probing provides a direct measure of the quality and generalization capability of the representations\n",
      "learned by CLIP. Strong performance from a linear classifier on specific tasks indicates that the\n",
      "pre-trained model has effectively captured relevant and discriminative features, underscoring the\n",
      "robustness of its embeddings. Additionally, we assess CLIP’s representation quality using LLaV A (Liu\n",
      "et al., 2023b) as a case study, where the vision encoder remains frozen during both pre-training and\n",
      "SFT stages. This makes LLaV A an ideal benchmark for evaluating the transferability and integrity of\n",
      "CLIP’s learned representations.\n",
      "We summarize the results on linear probing in Fig. 2: even though DSC and SSC shows lower\n",
      "zero-shot performance, they achieve comparable results to AltText after linear probing, indicating\n",
      "similar pre-trained representations. Furthermore, combining synthetic captions with AltText yields\n",
      "the best overall performance. Then we use these pre-trained vision encoders and insert them into\n",
      "LLaV A and complete the default pre-training and SFT stages in LLaV A (Liu et al., 2023b). All of\n",
      "our pre-trained CLIPs use ViT-B/16 as the backbone. We use Vicuna-1.3 as the LLM for LLaV A\n",
      "training and report recent benchmarks in Table A2: POPE (Li et al., 2023b), TextVQA (Singh et al.,\n",
      "2019b), GQA (Hudson & Manning, 2019), SciQA (Lu et al., 2022), LLaV A-Bench (Liu et al., 2023b),\n",
      "MME (Fu et al., 2024), and MM-Vet (Yu et al., 2024b). In this case, the combination of SSC and\n",
      "AltText achieves the highest overall performance, leading in 5 out of 9 columns. This highlights\n",
      "the critical role of both synthetic captions and original AltTexts in CLIP’s pre-training: synthetic\n",
      "captions enhance image-text alignment, while AltTexts introduce valuable data diversity.\n",
      "Compatibility of rewritten-based methods and filtering-based methods. Besides rewritten-based\n",
      "datasets like web-crawled VeCap-300M (Lai et al., 2024), which leverage recaptioning techniques to\n",
      "improve image-text alignment, it is essential to evaluate the compatibility between rewriting-based\n",
      "16\n",
      "Preprint\n",
      "Table A2: LLaV A as the benchmark for evaluating CLIP vision encoder pre-trained on different captions.\n",
      "Pre-trained CaptionPOPETextVQA GQASciQA LLaV A-Bench MMEMM-VetAvg. IMG Accuracy COCO Perception Cognition\n",
      "AltText 84.4 50.2 59.1 64.3 64.8 76.4 1332.6 271.1 25.0\n",
      "DSC 83.4 47.6 59.1 63.2 64.8 75.6 1307.8 312.9 23.9\n",
      "DSC + AltText 84.6 49.8 59.9 63.6 65.3 77.2 1405.3 273.2 25.5\n",
      "SSC 84.5 48.2 59.6 59.0 62.7 76.7 1343.6 248.6 22.4\n",
      "SSC + AltText 84.7 49.4 60.1 65.3 65.8 77.9 1370.7 270.7 25.1\n",
      "Table A3: Compatibility between rewritten-based and filtering-\n",
      "based methods. We use DFN-2B (Fang et al., 2023) as the example\n",
      "and train CLIP with ViT/B-16 on different captions.\n",
      "Pre-train CaptionCOCO (R@1) Flickr30k (R@1)ImageNet ImageNetV2I-T T-I I-T T-I\n",
      "AltText 61.22 49.27 86.20 68.94 76.12 68.49\n",
      "DSC 51.47 30.99 77.28 55.68 27.97 23.67\n",
      "SSC 59.06 31.91 87.41 62.49 53.96 46.39\n",
      "DSC+SSC 60.68 38.46 89.60 68.68 56.10 49.15\n",
      "AltText+DSC+SSC 70.56 50.74 92.40 76.92 72.45 64.98\n",
      "Figure A1: Effect of LiT (Zhai et al.,\n",
      "2022) on different captions after pre-\n",
      "training.\n",
      "and filtering-based methods that remove mismatched image-text pairs. We consider DFN-2B (Fang\n",
      "et al., 2023) as a representative example, where a pre-trained CLIP model filters the dataset to retain\n",
      "pairs that align well with CLIP’s capabilities. A key research direction is to explore whether synthetic\n",
      "captions can effectively replace the original CLIP-selected AltText. To investigate this, we apply our\n",
      "captioning pipeline to DFN-2B (Fang et al., 2023) to generate synthetic captions and pre-train CLIP\n",
      "models on these captions. The results are summarized in Table A3. In this CLIP-filtered dataset,\n",
      "neither rewritten DSC nor SSC, nor their combination, outperforms the original AltText. However,\n",
      "when combining all our synthetic captions with the original AltText (using uniform sampling during\n",
      "training), we observe significant improvements in retrieval tasks, such as +9.34% on COCO I-T\n",
      "and+7.98% on Flickr T-I tasks, demonstrating the enhanced image-text alignment provided by our\n",
      "synthetic captions. Nevertheless, incorporating synthetic captions results in a performance drop of\n",
      "around 4% on ImageNet, highlighting the crucial role of the diverse information contained in the\n",
      "original AltText for CLIP’s learning.\n",
      "Effect of Locked-image text Tuning (LiT) (Zhai et al., 2022) after pre-training with different\n",
      "captions. LiT (Zhai et al., 2022) trains a text model to derive meaningful representations from\n",
      "a pre-trained image model by freezing the image encoder and fine-tuning only the text encoder.\n",
      "We investigate the impact of LiT after pre-training CLIP on different captions, with the results\n",
      "summarized in Fig. A1. During the LiT stage, we continue to train the text encoder using AltText.\n",
      "Our findings reveal that LiT with AltText consistently benefits both SSC and TSC across all evaluation\n",
      "sets, highlighting once again the critical role of AltText in CLIP training.\n",
      "A.2 M ULTIMODAL LLM\n",
      "We summarize the training details in Table A4. For the pre-training stage, we pre-train models on up\n",
      "to 512 TPUs with JAX (Bradbury et al., 2018).\n",
      "Table A4: Pre-training hyper-parameters and settings for the Multimodal LLM experiments. We use the same\n",
      "configuration as the 1.2B model in MM1 (McKinzie et al., 2024).\n",
      "General\n",
      "Batch size 512\n",
      "Image encoder 336×336ViT-L/14\n",
      "Visual-language connector C-Abstractor with 144image tokens\n",
      "Language model 1.2B transformer decoder-only language model\n",
      "Steps 50000\n",
      "17\n",
      "Preprint\n",
      "For the SFT experiments, we follow the same datasets and configuration as in MM1 (McKinzie et al.,\n",
      "2024).\n",
      "TextCore. For the pre-training benchmarks, TextCore is an average number of 8 benchmarks:\n",
      "ARC (Clark et al., 2018), PIQA (Bisk et al., 2020), LAMBADA (Paperno et al., 2016), Wino-\n",
      "Grande (Sakaguchi et al., 2021), HellaSWAG (Zellers et al., 2019), SciQ (Welbl et al., 2017),\n",
      "TriviaQA (Joshi et al., 2017), and WebQS (Berant et al., 2013).\n",
      "SFT dataset. For the SFT benchmarks, we summarize the details in Table A5: we mainly use\n",
      "MME (Fu et al., 2024), SEED (Li et al., 2023a), POPE (Li et al., 2023b), LLaV A-Bench (Wild) (Liu\n",
      "et al., 2023b), MM-Vet (Yu et al., 2024b), TextVQA (Singh et al., 2019a), MMMU (Yue et al., 2023),\n",
      "MathVista (Lu et al., 2023), ScienceQA (Lu et al., 2022).\n",
      "Benchmark Metric\n",
      "MME (Fu et al., 2024) Normalized Accuracy\n",
      "SEED (Li et al., 2023a) Seed-IMG\n",
      "POPE (Li et al., 2023b) Average of random, popular and adversarial\n",
      "LLaV A-Bench (Wild) (Liu et al., 2023b) GPT-assisted score\n",
      "MM-Vet (Yu et al., 2024b) GPT-assisted score\n",
      "TextVQA (Singh et al., 2019a) VQA Open Flamingo Accuracy\n",
      "MMMU (Yue et al., 2023) Accuracy\n",
      "MathVista (Lu et al., 2023) GPT-assisted score\n",
      "ScienceQA (Lu et al., 2022) Accuracy-IMG\n",
      "Table A5: Details of benchmarks and their metrics used in MM1.\n",
      "A.3 D IFFUSION MODEL\n",
      "We summarize the training details of our self-implemented diffusion model based on Stable Diffusion\n",
      "3 (Esser et al., 2024) in Table A6.\n",
      "Table A6: Pre-training hyper-parameters for our diffusion model based on Stable Diffusion 3.\n",
      "Batch size 4096\n",
      "Image size 256×256\n",
      "Step 500,000\n",
      "Text condition in-house CLIP’s G/14 text encoder with T5 tokenizer (Raffel et al., 2020)\n",
      "Text maximum token length 77\n",
      "Optimizer Adafactor ( β1= 0.9, β2= 0.999)\n",
      "Learning rate (LR) 0.0001 (constant with linear warm-up for 1k steps)\n",
      "Ema decay 0.9999\n",
      "Classifier free guidance 7.5\n",
      "B A D EEPER ANALYSIS OF GENERATED CAPTIONS\n",
      "Fig. 5 is an overview of our two-stage fine-tuning process: we first convert a MLLM into an image\n",
      "captioner, then we further fine-tune it to convert it into a human-aligned captioner. Our smaller image\n",
      "captioning model (3B) efficiently generates large volumes of synthetic data for our experiments. Using\n",
      "this model, we re-captioned a dataset of 7 billion images across multiple iterations. Furthermore, our\n",
      "larger model, with 7 billion parameters, is designed to produce more detailed captions, surpassing the\n",
      "level of detail offered by our long caption format.\n",
      "AFC fine-tuning dataset. To generate AltText Fusion Captions (AFC), we also prepare a fine-tuning\n",
      "dataset in this format. Specifically, given AltText and a DSC caption generated by our captioner, we\n",
      "ask LLM to fuse AltText information to the DSC. By this way, we construct a 20K training dataset\n",
      "for our captioner.\n",
      "18\n",
      "Preprint\n",
      "Less hallucinations in our DSC. The Caption Hallucination Assessment with Image Relevance\n",
      "(CHAIR) metric (Rohrbach et al., 2018) is a custom-designed evaluation tool developed to identify\n",
      "and measure the extent of object hallucination in image captioning tasks. The metric determines the\n",
      "proportion of generated words that accurately correspond to objects present in the image, as verified\n",
      "by ground truth sentences and object segmentations. It includes two scores: one that measures the\n",
      "fraction of hallucinated object instances (referred to as CHAIR i), and the other that calculates the\n",
      "fraction of sentences containing at least one hallucinated object (referred to as CHAIR s):\n",
      "CHAIR i=|{hallucinated objects }|\n",
      "|{all objects mentioned }|,CHAIR s=|{sentences with hallucinated object }|\n",
      "|{all sentences }|.\n",
      "Table A7: Hallucination detection across different\n",
      "MLLMs and our captioner.\n",
      "Model CHAIR i(↓)CHAIR s(↓)\n",
      "InstructBLIP (Dai et al., 2023) 14.5 30.0\n",
      "MiniGPT-4 (Zhu et al., 2023) 8.2 24.2\n",
      "Shikra (Chen et al., 2023) 7.0 22.0\n",
      "LLaV A-1.5 (Liu et al., 2023b) 6.2 20.6\n",
      "Our 5.9 19.6As shown in Table A7, our model achieves\n",
      "a CHAIR iscore of 5.9 and a CHAIR sscore\n",
      "of 19.6, outperforming leading models such as\n",
      "LLaV A-1.5 (Liu et al., 2023b), Shikra (Chen\n",
      "et al., 2023), and MiniGPT-4 (Zhu et al., 2023).\n",
      "The lower CHAIR scores indicate that our cap-\n",
      "tioner produces fewer hallucinated objects per\n",
      "instance and fewer sentences containing hallu-\n",
      "cinated objects. This improvement shows the\n",
      "effectiveness of our two-stage fine-tuning pro-\n",
      "cess, which strategically reduces objects hallucination by leveraging human-aligned data and strict\n",
      "prompt constraints. Consequently, our model offers more reliable and accurate objects recognition\n",
      "capabilities, making it a robust tool for generating high-quality captions across various applications.\n",
      "ANA: Average Number of Assertions, a metric for evaluating richness of captions. Besides\n",
      "hallucination, the richness of captions are also an important index to control the generated captions.\n",
      "We propose ANA to quantify the richness of captions. Inspired by GenEval (Ghosh et al., 2024) for\n",
      "evaluating text-to-image generation models, we reverse the process of text-to-image to image-to-text.\n",
      "As shown in Fig. A2, we prompt an LLM to generate different assertions of a caption. After that, we\n",
      "can also leverage a VQA model to check if these details are aligned with the visual contents.\n",
      "CapScore: A metric for evaluating hallucinations in synthetic captions. Although the CHAIR\n",
      "metric is widely used, we find it insufficient for detecting hallucinations in object attributes, especially\n",
      "in highly descriptive captions. To overcome this limitation, we propose a new metric to evaluate both\n",
      "the hallucination and richness of captions. Inspired by GenEval (Ghosh et al., 2024) for evaluating\n",
      "text-to-image generation models, we reverse the process of text-to-image to image-to-text and propose\n",
      "CapScore. CapScore measures the correctness of synthetic captions by evaluating the alignment\n",
      "between generated textual assertions and the actual content of the image. As shown in Fig. A2,\n",
      "CapScore has two key steps: 1) use an LLM to extract structured assertions from the captions; 2) use\n",
      "a MLLM to serve as a VQA (Visual Question Answering) model to verify the truthfulness of these\n",
      "assertions. Specifically, each assertion represents a distinct factual claim made within the caption.\n",
      "Then the VQA model determines whether the image supports each claim by answering questions\n",
      "based on the assertions. CapScore is then defined as the percentage of assertions validated as correct\n",
      "by the VQA model. A higher CapScore indicates fewer hallucinations and greater factual accuracy in\n",
      "the generated captions.\n",
      "As shown in Table A8, there is a notable trade-off between the richness of captions and their accuracy.\n",
      "As captions become longer, the Average Number of Assertions (ANA) increases, reflecting the\n",
      "growing richness and complexity of the generated captions. However, CapScore drops with longer\n",
      "captions, which suggests that while the captions provide more content, they are more prone to\n",
      "hallucinations—where the captioning model introduces incorrect or irrelevant information not present\n",
      "in the image. For instance, while DSC+ produces the highest ANA, it also demonstrates the lowest\n",
      "CapScore, highlighting this balance. By contrast, SSC maintains a higher CapScore with fewer\n",
      "assertions, demonstrating better alignment with the image content but at the cost of less detailed\n",
      "descriptions.\n",
      "This behavior highlights the importance of balancing richness and accuracy in multimodal tasks.\n",
      "Models aiming for high-precision applications (e.g., zero-shot classification with CLIP) may benefit\n",
      "from shorter captions (e.g., SSC), whereas scenarios requiring more detailed scene descriptions (e.g.,\n",
      "19\n",
      "Preprint\n",
      "Figure A2: An overview of CapScore to evaluate the quality of captions: we use LLM to generate\n",
      "assertions based on the caption and then use a VQA model to check these assertions.\n",
      "mutlimodal LLMs) may prioritize longer captions like DSC+, accepting some decrease in factual\n",
      "accuracy.\n",
      "Table A8: CapScore and Average Number of Assertions (ANA) to evaluate the richness and accuracy of\n",
      "different captions.\n",
      "Caption CapScore (↑)ANA (↑)\n",
      "LLaV A-1.5 (Liu et al., 2023b) 88.76 7.30\n",
      "SSC 91.56 2.49\n",
      "DSC 87.30 8.13\n",
      "DSC+ 75.74 12.20\n",
      "More detailed but noisy captions are helpful for MLLM pre-training. We examine the impact of\n",
      "hallucinations in image-text data used for MLLM pre-training, focusing on the type of hallucinations\n",
      "detected in our captions. For our primary comparison, we select the best-performing model, LLaV A-\n",
      "1.5 (Liu et al., 2023b), as shown in Table A7. We first utilize LLaV A-1.5 to generate captions on the\n",
      "VeCap-300M dataset (Lai et al., 2024). Next, we apply our captioner to generate DSC+ for the same\n",
      "set of images. Our captions are more detailed but contain more hallucinations on object. We use\n",
      "MM1’s pre-training under a small scale setting as a case study to examine whether image-caption\n",
      "data with fewer hallucinations or with more details can offer advantages for MLLM pre-training. We\n",
      "evaluate the two models after applying fine-tuning using the same data recipe.\n",
      "As shown in Table A9, the MLLM pre-trained with more detailed captions performs better, even\n",
      "though these data contain more hallucinations. When examining task-specific results, we observe\n",
      "that our DSC+ pre-trained model outperforms the LLaV A captions model on 7 out of 9 benchmarks.\n",
      "Specifically, DSC+ improves performance on VQAv2, MMMU, MathV , and SEED, among oth-\n",
      "ers, indicating that the added detail from DSC+ benefits these tasks. However, despite the slight\n",
      "degradation in MMEP results (727.2 vs. 773.4), the overall advantage of detailed captions with\n",
      "minor hallucinations demonstrates that a balance between information richness and accuracy can\n",
      "positively impact MLLM’s performance. The larger gains in multimodal understanding tasks, such as\n",
      "MMEC and LLaV AW, suggest that hallucination-tolerant MLLM pre-training may help with complex\n",
      "vision-language reasoning.\n",
      "Table A9: SFT evaluation of models pre-trained with captions generated by LLaV A-1.5 and our DSC+. We use\n",
      "the same SFT recipe and evaluation benchmarks as in MM1 (McKinzie et al., 2024).\n",
      "Pre-Trained Data VQAv2 VQATMMMU MathV MMEPMMECSEED POPE LLaV AW\n",
      "LLaV A Captions 78.0 65.4 30.0 27.6 773.4 200.4 63.5 83.7 63.6\n",
      "DSC+ 79.0 65.4 32.6 29.4 727.2 224.3 66.8 85.1 71.5\n",
      "20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pdf_dict['Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using Llama 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import os\n",
    "import os\n",
    "\n",
    "# Import HuggingFace\n",
    "os.environ['HF_HOME'] = r'C:\\Users\\josha\\AppData\\Local\\Temp'    # Set cache directory for HuggingFace\n",
    "from transformers import (AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline)\n",
    "\n",
    "# Import json\n",
    "import json\n",
    "\n",
    "# Import torch\n",
    "import torch\n",
    "\n",
    "# Import numpy\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HF account config\n",
    "config_data = json.load(open(\"config.json\"))\n",
    "HF_TOKEN = config_data[\"HF_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model name\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantisation config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0e92e714cfb49b5a2d91472348cbee1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\josha\\AppData\\Local\\Temp\\hub\\models--meta-llama--Meta-Llama-3-8B. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae33a6d58f44b869e6479f0c2ac6ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\josha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:983: UserWarning: Not enough free disk space to download the file. The expected file size is: 4976.70 MB. The target location C:\\Users\\josha\\AppData\\Local\\Temp\\hub\\models--meta-llama--Meta-Llama-3-8B\\blobs only has 2113.80 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2da8631000458c96be28f59ef0c5a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,                     # Address for Llama3\n",
    "    device_map=\"auto\",              # Loads model in GPU if available, else CPU\n",
    "    quantization_config=bnb_config, # Reduces precision of model params to reduce model size\n",
    "    use_auth_token=HF_TOKEN         # Private token to access model\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
