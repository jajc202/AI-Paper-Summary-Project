{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Experimentation V2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Paper Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the paperscraper module\n",
    "from paperscraper import paperscraper\n",
    "\n",
    "# Create an instance of the class\n",
    "scraper = paperscraper('2024-10-04')\n",
    "\n",
    "# Get sections of AIAYN paper using html_subdivide function\n",
    "AIAYN_sections = scraper.html_subdivide('https://arxiv.org/html/1706.03762v7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summarisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "# Import os\n",
    "import os\n",
    "\n",
    "# Import GPT model and tokenizer\n",
    "os.environ['HF_HOME'] = r'C:\\Users\\josha\\AppData\\Local\\Temp'        # Set cache directory for HuggingFace\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration    # Using Google FLAN-T5\n",
    "\n",
    "# Create an instance of the tokenizer\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Create an instance of the model\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summarisation function\n",
    "def section_summarizer(text):\n",
    "    # Define prompt\n",
    "    prompt = f'''You are an AI assistant capable of summarizing academic content on AI and Machine Learning for non-academic readers. Your summaries are given in the third-person, and are about 2-3 sentences in length.\n",
    "    Summarize the following abstract:\n",
    "    {text}'''\n",
    "\n",
    "    # Encode prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "    # Generate response\n",
    "    output = model.generate(\n",
    "        input_ids,                  # Input ids\n",
    "        max_length = 500,           # Maximum number of tokens to generate\n",
    "        num_beams = 5,              # Needed as next token is found using beam search\n",
    "        no_repeat_ngram_size = 2,   # Stops model from repeating word sequences repeatedly\n",
    "        early_stopping = False,      # If output becomes not very good, stop generating\n",
    "    )\n",
    "\n",
    "    # Return the decoded the output\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (6070 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "summarised_sections = {}\n",
    "\n",
    "for section, content in AIAYN_sections.items():\n",
    "    summarised_sections[section] = section_summarizer(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abstract': 'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2.',\n",
       " 'Introduction': 'We propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.',\n",
       " 'Background': 'The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution.',\n",
       " 'Model Architecture': 'We propose an encoder-decoder architecture based on a multi-head self-attention and point-wise fully connected feed-forward networks.',\n",
       " 'Encoder and Decoder Stacks': 'We employ a residual connection around each of the two sub-layers, followed by layer normalization. We also modify the self-attention sublayer in the decoder stack to prevent positions from attending to subsequent positions.',\n",
       " 'Attention': 'We propose a multi-head attention function for encoding and decoders.',\n",
       " 'Position-wise Feed-Forward Networks': 'Our encoder and decoder contain a fully connected feed-forward network, which is applied to each position separately and identically. The linear transformations are the same across different positions, and they use different parameters from layer to layer.',\n",
       " 'Embeddings and Softmax': 'We use learned embeddings to convert decoder output to predicted next-token probabilities.',\n",
       " 'Positional Encoding': 'We add positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks.',\n",
       " 'Why Self-Attention': 'We compare self-attention layers to the recurrent and convolutional layers for learning a variable-length sequence of symbol representations.',\n",
       " 'Training': 'We train our models on the standard WMT 2014 English-German dataset using byte-pair encoding[3].',\n",
       " 'Training Data and Batching': 'We trained on the standard WMT 2014 English-German dataset using byte-pair encoding[3].',\n",
       " 'Hardware and Schedule': 'We trained our models on one machine with 8 NVIDIA P100 GPUs. Each training step took about 0.4 seconds.',\n",
       " 'Optimizer': 'We used the Adam optimizer[20] with 1=0.9 subscript10.9beta_ 1=0.7italic start_POSTSUBSCRIPT 1 end_PSTSBRACPIT = 0.9,2 =0.98bd model20.98bytesate=dmodel0.5min(step_num 0.5,sstep\\xadnum1.5)lratesuperscriptsubscriptdModel0.5step_nuSuperscriptm0.5>step numwarmup_step superscripts1.5l rate=textmodel[d]x1.0cdot-min(ascii_s[0][1]for x,y,z,p,t,e,i,j,k,l,r,m,n,u,italic_p[2] for y and z) for the first warmup steps of training.',\n",
       " 'Regularization': 'We apply dropout[33]to the output of each sub-layer, before it is added to the sublayer input and normalized. For the base model, we use a rate ofPdrop=0.1subscriptPdrop0.1P_drop=0.2.',\n",
       " 'Results': 'The Transformer outperforms RNN sequence-to-sequence models in English constituency parsing.',\n",
       " 'Machine Translation': 'The Transformer achieves better BLEU scores than previous state-of-the-art models, at a fraction of the training cost. On the WMT 2014 English-to-German translation task, our big transformer model outperforms all previously published models.',\n",
       " 'Model Variations': 'Table 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. We present our results in Table3. In Table3rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.',\n",
       " 'English Constituency Parsing': 'The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ)ParserTraining.',\n",
       " 'Conclusion': 'We present the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks.',\n",
       " 'References': '[1]Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.Neural machine translation by jointly learning to align and translate.CoRR, abs/1409.0473, 2014.[3]Denny Britz, Anna Goldie, Minh-Thang Luong,',\n",
       " 'Attention Visualizations': 'We present two examples of the attention mechanism following long-distance dependencies in the encoder self-attention at layer 5 of 6.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarised_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTML file saved as output.html\n"
     ]
    }
   ],
   "source": [
    "def dict_to_html(data_dict):\n",
    "    html_output = \"<html>\\n<head>\\n<title>Dictionary Output</title>\\n</head>\\n<body>\\n\"\n",
    "    \n",
    "    for key, value in data_dict.items():\n",
    "        html_output += f\"<h2>{key}</h2>\\n<p>{value}</p>\\n\"\n",
    "    \n",
    "    html_output += \"</body>\\n</html>\"\n",
    "    \n",
    "    return html_output\n",
    "\n",
    "# Generate HTML\n",
    "html_content = dict_to_html(summarised_sections)\n",
    "\n",
    "# Save to an HTML file\n",
    "with open(\"output.html\", \"w\") as file:\n",
    "    file.write(html_content)\n",
    "\n",
    "print(\"HTML file saved as output.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Focusing on Abstracts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'NoneType' object has no attribute 'get'\n",
      "An error occurred: 'NoneType' object has no attribute 'get'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models': 'https://arxiv.org/html/2410.02740v1',\n",
       " 'SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration': 'https://arxiv.org/html/2410.02367v1',\n",
       " 'Depth Pro: Sharp Monocular Metric Depth in Less Than a Second': 'https://arxiv.org/html/2410.02073v1',\n",
       " 'Loong: Generating Minute-level Long Videos with Autoregressive Language Models': 'https://arxiv.org/html/2410.02757v1',\n",
       " 'LLaVA-Critic: Learning to Evaluate Multimodal Models': 'https://arxiv.org/html/2410.02712v1',\n",
       " 'Video Instruction Tuning With Synthetic Data': 'https://arxiv.org/html/2410.02713v2',\n",
       " 'Large Language Models as Markov Chains': 'https://arxiv.org/html/2410.02724v1',\n",
       " 'Contrastive Localized Language-Image Pre-Training': 'https://arxiv.org/html/2410.02746v1',\n",
       " 'VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment': 'https://arxiv.org/html/2410.01679v1',\n",
       " 'Distilling an End-to-End Voice Assistant Without Instruction Training Data': 'https://arxiv.org/html/2410.02678v1',\n",
       " 'CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling': 'https://arxiv.org/html/2409.19291v2',\n",
       " 'Contextual Document Embeddings': 'https://arxiv.org/html/2410.02525v3',\n",
       " 'Training Language Models on Synthetic Edit Sequences Improves Code Synthesis': 'https://arxiv.org/html/2410.02749v2',\n",
       " 'Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations': 'https://arxiv.org/html/2410.02762v1',\n",
       " 'Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models': 'https://arxiv.org/html/2410.01782v1',\n",
       " 'MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation': 'https://arxiv.org/html/2410.02458v2',\n",
       " 'Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning': 'https://arxiv.org/html/2410.02052v3',\n",
       " 'MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis': 'https://arxiv.org/html/2410.02103v1',\n",
       " 'Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos': 'https://arxiv.org/html/2410.02763v1',\n",
       " 'Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models': 'https://arxiv.org/html/2410.01335v1',\n",
       " 'Intelligence at the Edge of Chaos': 'https://arxiv.org/html/2410.02536v2',\n",
       " 'Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning': 'https://arxiv.org/html/2410.00255v1',\n",
       " 'Learning the Latent Rules of a Game from Data: A Chess Story': 'https://arxiv.org/html/2410.02426v1',\n",
       " 'SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics': 'https://arxiv.org/html/2410.01946v1',\n",
       " 'Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data': 'https://arxiv.org/html/2410.02056v1'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the class\n",
    "scraper = paperscraper('2024-10-04')\n",
    "\n",
    "# Get dictionary of all html links for date using paperscraper class\n",
    "pdf_pages = scraper.get_links()\n",
    "pdf_links = scraper.get_html_links(pdf_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to summarise abstract of a paper\n",
    "def abstract_summary(link):\n",
    "    # Get dictionary of paper sections\n",
    "    html_sections = scraper.html_subdivide(link)\n",
    "\n",
    "    if \"Abstract\" in html_sections:\n",
    "        # Define prompt\n",
    "        prompt = f'''You are an AI assistant capable of summarizing academic content on AI and Machine Learning for non-academic readers. Your summaries are given in the third-person, and are about 2-3 sentences in length.\n",
    "        Summarize the following abstract:\n",
    "        {html_sections[\"Abstract\"]}'''\n",
    "\n",
    "        # Encode prompt\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "\n",
    "        # Generate response\n",
    "        output = model.generate(\n",
    "            input_ids,                  # Input ids\n",
    "            max_length = 500,           # Maximum number of tokens to generate\n",
    "            num_beams = 5,              # Needed as next token is found using beam search\n",
    "            no_repeat_ngram_size = 2,   # Stops model from repeating word sequences repeatedly\n",
    "            early_stopping = False,      # If output becomes not very good, stop generating (if set to True)\n",
    "        )\n",
    "\n",
    "        # Return the decoded the output\n",
    "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    else:\n",
    "        print(\"Paper has no Abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarised_abstracts = {}\n",
    "\n",
    "for paper, link in pdf_links.items():\n",
    "    summarised_abstracts[paper] = abstract_summary(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Revisit Large-Scale Image-Caption Data in Pre-training Multimodal Foundation Models': 'We propose a novel, controllable, and scalable captioning pipeline designed to generate diverse caption formats tailored to various multimodal models.',\n",
       " 'SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration': 'We propose SageAttention, a highly efficient and accurate quantization method for attention.',\n",
       " 'Depth Pro: Sharp Monocular Metric Depth in Less Than a Second': 'Depth Pro is a foundation model for zero-shot metric monocular depth estimation.',\n",
       " 'Loong: Generating Minute-level Long Videos with Autoregressive Language Models': 'We propose Loong, a new autoregressive LLM-based video generator that can generate minute-long videos.',\n",
       " 'LLaVA-Critic: Learning to Evaluate Multimodal Models': 'We introduce LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator.',\n",
       " 'Video Instruction Tuning With Synthetic Data': 'We propose a high-quality synthetic video instruction-following dataset, LLaVA-Video-178K.',\n",
       " 'Large Language Models as Markov Chains': 'We draw an equivalence between generic autoregressive language models with a vocabulary of sizeTTTitalic_Tand context window and Markov chains defined on an finite state space. We then prove pre-training and in-context generalization bounds and show how we enrich their interpretation. Finally, we illustrate our theoretical guarantees with experiments on several recent LLMs to highlight how they capture the behavior observed in practice.',\n",
       " 'Contrastive Localized Language-Image Pre-Training': 'We improve the localization capability of Contrastive Language-Image Pre-training by complementing CLIP with region-text contrastive loss and modules.',\n",
       " 'VinePPO: Unlocking RL Potential For LLM Reasoning Through Refined Credit Assignment': 'We propose VinePPO, a straightforward approach that leverages the flexibility of language environments to compute unbiased Monte Carlo-based estimates, bypassing the need for large value networks.',\n",
       " 'Distilling an End-to-End Voice Assistant Without Instruction Training Data': 'Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only language model to transcripts as self-supervision.',\n",
       " 'CLIP-MoE: Towards Building Mixture of Experts for CLIP with Diversified Multiplet Upcycling': 'Diversified Multiplet Upcycling integrates multiple CLIP models into a Mixture of Experts architecture.',\n",
       " 'Contextual Document Embeddings': 'We propose two complementary methods for contextualized document embeddings: an alternative contrastive learning objective and a new contextual architecture that explicitly encodes neighbor document information into the encoded representation.',\n",
       " 'Training Language Models on Synthetic Edit Sequences Improves Code Synthesis': 'We develop a synthetic data generation algorithm called LintSeq that refactors existing code into sequence of code edits by using alinterto procedurally sample across the error-free insertions that can be used to sequentially write programs. We show that during repeated sampling, edit sequence finetuned models produce more diverse programs than baselines, i.e. the fraction of problems “pass@k” solved by any attempt given \"k\" tries. Finally, we pretrain our own tiny LMs for code understanding.',\n",
       " 'Interpreting and Editing Vision-Language Representations to Mitigate Hallucinations': 'We propose a knowledge erasure algorithm that reduces hallucinations by up to 25% on the COCO2014 dataset while preserving performance.',\n",
       " 'Open-RAG: Enhanced Retrieval-Augmented Reasoning with Open-Source Large Language Models': 'Open-RAG is a novel framework designed to enhance reasoning capabilities in RAG with open-source LLMs.',\n",
       " 'MedVisionLlama: Leveraging Pre-Trained Large Language Model Layers to Enhance Medical Image Segmentation': 'A Hybrid Attention Mechanism combines global and local feature learning with a Multi-Scale Fusion Block for aggregating features across different scales.',\n",
       " 'Improving Autonomous AI Agents with Reflective Tree Search and Self-Learning': 'We present ExAct, an approach to combine test-time search and self-learning to build o1-like models for agentic applications.',\n",
       " 'MVGS: Multi-view-regulated Gaussian Splatting for Novel View Synthesis': 'We propose a new 3DGS optimization method that improves novel view synthesis of the Gaussian-based explicit representation methods about 1 dB PSNR for various tasks.',\n",
       " 'Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos': 'We introduce Vinoground, a temporal counterfactual LMM evaluation benchmark encompassing 1000 short and natural video-caption pairs.',\n",
       " 'Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language Models': 'We present a model merging methodology that addresses the difficulty of fine-tuning Large Language Models for target tasks in non-English languages, where task-specific data is often unavailable. We focus on mathematical reasoning and without in-language math data, facilitate cross-lingual transfer by composing language and math capabilities.',\n",
       " 'Intelligence at the Edge of Chaos': 'We explore the emergence of intelligent behavior in artificial systems by investigating how the complexity of rule-based systems influences the abilities of models trained to predict these rules.',\n",
       " 'Robin3D: Improving 3D Large Language Model via Robust Instruction Tuning': 'We introduce Robin3D, a powerful 3D Large Language Model trained on large-scale instruction-following data generated by our novel data engine, Robust Instruction Generation (RIG).',\n",
       " 'Learning the Latent Rules of a Game from Data: A Chess Story': 'We demonstrate that small pretrained foundational generative language models with millions of parameters can learn the latent rules of a process from data associated with the process.',\n",
       " 'SciPrompt: Knowledge-augmented Prompting for Fine-grained Categorization of Scientific Topics': 'We introduce SciPrompt, a framework designed to automatically retrieve scientific topic-related terms for low-resource text classification tasks.',\n",
       " 'Synthio: Augmenting Small-Scale Audio Classification Datasets with Synthetic Data': 'We presentSynthio, a novel approach for augmenting small-scale audio.'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarised_abstracts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
